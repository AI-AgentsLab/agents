{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9641c10f",
   "metadata": {},
   "source": [
    "# Generate a business idea with Amazon Bedrock.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef3b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "# This will also update your pyproject.toml and uv.lock files. \n",
    "!uv add boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b57a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from time import sleep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505a930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load api key from .env or environment variable. This notebook is using the simpler API key method, which gives access only to Amazon Bedrock services, instead of standard AWS credentials\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "os.environ['AWS_BEARER_TOKEN_BEDROCK'] = os.getenv('AWS_BEARER_TOKEN_BEDROCK', 'your-key-if-not-using-env')\n",
    "\n",
    "region = 'us-east-1' # change to your preferred region - be aware that not all regions have access to all models. If in doubt, use us-east-1.\n",
    "\n",
    "bedrock = boto3.client(service_name=\"bedrock\", region_name=region) # use this for information and management calls (such as model listings)\n",
    "bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\", region_name=region) # this is for inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2617043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do a quick test to see if works.\n",
    "# We will list the available models.\n",
    "\n",
    "response = bedrock.list_foundation_models()\n",
    "models = response['modelSummaries']\n",
    "print(f'AWS Region: {region} - Models:')\n",
    "for model in models:\n",
    "    print(f\"Model ID: {model['modelId']}, Name: {model['modelName']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b30ff6",
   "metadata": {},
   "source": [
    "### Amazon Bedrock Cross-Region Inference\n",
    "We will use Amazon Nova models for this example.  \n",
    "  \n",
    "For inference, we will be using the cross-region inference feature of Amazon Bedrock, which routes the inference call to the region which can best serve it at a given time.  \n",
    "Cross-region inference [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/cross-region-inference.html)  \n",
    "For the latest model names using cross-region inference, refer to [Supported Regions and models](https://docs.aws.amazon.com/bedrock/latest/userguide/inference-profiles-support.html)  \n",
    "\n",
    "**Important: Before using a model you need to be granted access to it from the AWS Management Console.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be42713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model and message\n",
    "# Amazon Nova Pro is a multimodal input model - it can be prompted with images and text. We'll only be using text here.\n",
    "\n",
    "QUESTION = [\"I want you to help me pick a business area or industry that might be worth exploring for an Agentic AI opportunity.\",\n",
    "            \"Expand on a pain point in that industry that is challenging and ready for an agentic AI solution.\",\n",
    "            \"Based on that idea, describe a possible solution\"]\n",
    "\n",
    "BEDROCK_MODEL_ID = 'us.amazon.nova-pro-v1:0'\n",
    "messages=[]\n",
    "\n",
    "system = [{\"text\": \"You are a helpful business consultant bot. You will respond in a professional, succinct tone in no more than 4 sentences.\"}]\n",
    "for i in range(3):\n",
    "    messages.append({\"role\": \"user\", \"content\": [{\"text\": QUESTION[i]}]})\n",
    "\n",
    "    # Make the API call\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=BEDROCK_MODEL_ID,\n",
    "        messages=messages,\n",
    "        system=system\n",
    "    )\n",
    "\n",
    "    # Store the response\n",
    "    answer = response['output']['message']['content'][0]['text']\n",
    "\n",
    "    # Store it into message history\n",
    "    assistant_message = {\"role\": \"assistant\", \"content\":[{\"text\":answer}]}\n",
    "    messages.append(assistant_message)\n",
    "    print(f\"{i}-Question: \"+QUESTION[i]+\"\\nAnswer: \" + answer)\n",
    "    sleep(5) # Trying to be nice to avoid hitting rate limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36c0e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
